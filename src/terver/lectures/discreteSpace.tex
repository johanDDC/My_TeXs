\section{Дискретное вероятностное пространство.}
%    \addcontentsline{toc}{section}{Лекция № 1.}

\subsection{Вероятностое пространство, события, вероятностная мера, вероятность.}
\begin{definition}
    Пусть задано некоторое множество возможных исходов $\Omega = \{\omega_1, \ldots, \omega_n\}$. Это множество называется
    \it{множеством элементарных исходов}. Всякое подмножество $A \subseteq \Omega$ называют \it{событием}.

    Функцию $P \colon 2^{\Omega} \to [0, 1]$, удовлетворяющую следующим свойствам:
    \begin{itemize}
        \item $P(\Omega) = 1$
        \item $A \cap B = \nothing \implies P(A \cup B) = P(A) + P(B)$ (аддитивность)
    \end{itemize}
    называют \it{вероятностной мерой}, а значение $P(A)$ \it{вероятностью} события $A$.
\end{definition}
Для всякого $\omega \in \Omega$ определена вероятностная мера: $P(\{ \omega \}) = p_{\omega}$. Из определения вероятностной меры
следует, что $0 \leq p_{\omega} \leq 1$. Тогда
\begin{equation*}
    \sum\limits_{\omega \in \Omega}p_{\omega} = \sum\limits_{k = 1}^{n} P(\{\omega_k\}) = P(\Omega) = 1
\end{equation*}
И тогда вероятность произвольного события $A$ вычисляется по формуле
\begin{equation*}
    P(A) = \sum\limits_{\omega \in A}P(\{\omega\}) = \sum\limits_{\omega \in A}p_{\omega}
\end{equation*}
Если все элементарные исходы равновозможны, то $p_{\omega_1} = \ldots = p_{\omega_n} = \frac{1}{n}$.
В этом случае вероятность события $A$ равна
\begin{equation*}
    P(A) = \sum\limits_{\omega \in A}p_{\omega} = \sum\limits_{k=1}^{|A|} \frac{1}{n} = \frac{|A|}{n}
\end{equation*}
\begin{problem}[Вероятностный алгоритм проверки числа на простоту.]
    Дано натуральное число $N > 1$. Хотим знать, является ли оно простым быстрее чем за $O(\sqrt{N})$.

    \it{Решение:} По малой теореме Ферма:\\
    если $N$ --- простое число, то для всякого взаимнопростого с $N$
    числа $b$ (НОД$(N, b) = 1$) число $b^{N - 1} - 1$ делится на $N (\iff b^{N - 1} \equiv 1 \mod N)$.\\
    Предложим следующий алгоритм: выбираем число $b$ случайным образом из промежутка $[2, N - 1]$. Если
    НОД$(b, N) \neq 1$, то $N$, очевидно, составное. Если НОД$(b, N) = 1$, но $b^{N - 1} - 1$ не делится на $N$,
    то $N$ не простое по малой теореме Ферма. Во всех остальных случаях будем считать, что $N$ простое. Заметим,
    что пока в нашем алгоритме используются только рандомный выбор (считаем, что можно сделать за константу),
    проверка делимости (тоже за константу), и подсчёт НОДа (ассимптотика алгоритма Евклида равна
    $O(\log\min(N, b))$ --- быстрее чем полином). Будем говорить, что "число $N$ проходит тест по основанию
    $b$"\,, если наш алгоритм определяет число $N$ как простое при выборе случайного $b$ взаимно простого с $N$.

    Предположим теперь, что есть хотя бы одно число $a$, такое что НОД$(a, N) = 1$, но $a^{N - 1}$ не делится на
    $N$. Посчитаем вероятность, с которой наш алгоритм выдаст, что $N$ --- простое (т.е. посчитаем вероятность
    ошибки алгоритма):\\
    Пусть $\ZZ_N^{*}$ --- группа всех взаимнопростых с $N$ чисел из промежутка $[1, N-1]$. Если $N$ проходит
    тест по основанию $b \in \ZZ_N^{*}$, то по основанию $ab$ (где $a \in \ZZ_N^{*}$) оно тест не пройдёт, т.к.
    \[
        \begin{cases}
            (ab)^{N - 1} \equiv 1 \mod N\\
            (b^{-1})^{N - 1} \equiv 1 \mod N
        \end{cases}
        \implies
        a^{N - 1} \equiv 1 \mod N
    \]
    Таким образом, для всякого основания $b$, по которому $N$ проходит тест, существует основание $ab$,
    на котором $N$ тест не проходит, а значит оснований, на которых $N$ не проходит тест не меньше чем тех,
    на которых $N$ тест проходит, а значит вероятность ошибки нашего алгоритма $\frac{1}{2}$. Выбирая число $b$
    случайным образом $k$ раз можно снизить вероятность ошибки алгоритма до $\frac{1}{2^k}$.
\end{problem}

\subsection{Формула включений и исключений.}
Пусть $\Omega$ конечное множество элементарных исходов, а $A_1, \cdots, A_n$ --- произвольные события.
\begin{proposal}
    Верно следующее равенство:
    \[
        P\left( \bigcup\limits_{k = 1}^{n}A_k \right) = \sum\limits_{k = 1}^{n}(-1)^{k-1}
        \sum\limits_{i_1<\ldots<i_k} P(A_{i_1} \cap \ldots \cap A_{i_k})
    \]
\end{proposal}
\begin{example}
    Пусть у нас есть множества $A_1, A_2, A_3$. Распишем для них формулу включений и исключений, чтобы разобраться в её записи.
    Напомню, что запись $\sum\limits_{i < j}$ эквивалентна записи $\sum\limits_{j=1}^n\sum\limits_{i=2}^j$.
    \begin{equation*}
        P(A_1 \cup A_2 \cup A_3) = P(A_1) + P(A_2) + P(A_3) - P(A_1 \cap A_2) - P(A_1 \cap A_3) - P(A_2 \cap A_3)
        + P(A_1 \cap A_2 \cap A_3)
    \end{equation*}
\end{example}
\begin{proof}
    Индукция по $n$:\\
    База: $n = 2$
    \begin{align*}
        P(A_1 \cup A_2) = P((A_1 \setminus A_2) &\cup (A_1 \cap A_2) \cup (A_2 \setminus A_1)) =\\
        &=P(A_1 \setminus A_2) + P(A_1 \cap A_2) + P(A_2 \setminus A_1) =
        P(A_1) + P(A_2) - P(A_1 \cap A_2)
    \end{align*}
    Предположение индукции: пусть выполнено для $n = k$ множеств.\\
    Шаг: проверим для $n = k + 1$ множеств
    \begin{align*}
        P(A_1 \cup \ldots \cup A_{k + 1}&) =
        \left|
        B = \bigcup\limits_{i = 1}^k A_i
        \right| = P(B \cup A_{k + 1}) = P(B) + P(A_{k + 1}) - P(B \cap A_{k + 1}) = \Diamond\\
        P(B) = P(A_1 \cup A_2 &\cup \ldots \cup A_k) = \sum\limits_{j = 1}^{k}(-1)^{j-1}
        \sum\limits_{i_1<\ldots<i_j} P(A_{i_1} \cap \ldots \cap A_{i_j})\\
        P(B \cap A_{k + 1}) = P((&A_1 \cup A_2 \cup \ldots \cup A_k) \cap A_{k + 1})
        =P((A_1 \cap A_{k + 1}) \cup \ldots \cup (A_k \cap A_{k + 1})) =\\
        &=\sum\limits_{i = 1}^k (-1)^{i - 1}\sum\limits_{j_1 < \ldots < j_i} P(A_{j_1} \cap \ldots \cap A_{j_i} \cap A_{k + 1})\\
        \Diamond = \sum\limits_{k = 1}^{n}(-1)^{k-1}
        \sum\limits_{i_1<\ldots<i_k}& P(A_{i_1} \cap \ldots \cap A_{i_k}) + P(A_{k + 1}) -
        \sum\limits_{i = 1}^k (-1)^{i - 1}\sum\limits_{j_1 < \ldots < j_i} P(A_{j_1} \cap \ldots \cap A_{j_i} \cap A_{k + 1})=\\
        &=\sum\limits_{j = 1}^{k + 1}(-1)^{j-1}
        \sum\limits_{i_1<\ldots<i_j} P(A_{i_1} \cap \ldots \cap A_{i_j})
    \end{align*}
\end{proof}

\subsection{Парадокс распределения подарков.}
$n$ человек решили подарить друг другу подарки по следующей схеме: каждый человек купил один подарок и положил его в мешок.
После этого все люди одновременно сунули руки в мешок и каждый вытащил себе наугад ровно один подарок.
\begin{enumerate}
    \item Какова вероятность, что каждый человек вытащил подарок, который сам и принёс?
    \item Какова вероятность того, что никто не вытащил подарок, который сам принёс?
\end{enumerate}

\it{Решение:} Занумеруем все подарки числами от $1$ до $n$. Таким образом мы можем думать о подарках как о перестановках
на $n$ элементах, а о событии, при котором каждый человек вытащил конкретно тот подарок, который принёс --- как о
тождественной перестановке. Пусть событие $A$ --- человек вытащил подарок, который сам принёс. Тогда один из $n$
подарков фиксирован, а все остальные переставляются случайным образом. Т.е. $|A| = (n - 1)!$, что соответствует
перестановкам остальных подарков, а $P(A) = \frac{(n - 1)!}{n!} = \frac{1}{n}$. Очевидно, что при достаточных $n$ эта вероятность
стремится к $0$.\\
Пусть теперь $A_k$ --- событие, соответствующее тому, что $k$-ый подарок попал к человеку, который его принёс. Тогда
$\bigcup\limits_{k = 1}^n A_k$ --- событие, соответсвующее тому, что хотя бы один подарок попал к человеку, который его
принёс. Тогда по формуле включений и исключений:
\[
    P\left( \bigcup\limits_{k = 1}^n A_k \right) = \sum\limits_{k = 1}^n (-1)^{k - 1}
    \sum\limits_{i_1 < \ldots < i_k} P(A_{i_1} \cap \ldots \cap A_{i_k})
\]
Заметим теперь, что вероятность $P(A_{i_1} \cap \ldots \cap A_{i_k})$ на русский язык переводится как "$i_1$ подарок достался
своему человеку, и $i_2$ подарок достался своему человеку, и \ldots, и $i_k$ подарок достался своему человеку". Т.е. $k$ подарков
досталось тем, кто их принёс. Т.е. $k$ подарков фиксированы, а остальные могут быть переставлены как угодно. Таким образом
вероястность события $A_{i_1} \cap \ldots \cap A_{i_k}$ равна $\frac{(n - k)!}{n!}$.\\
Теперь заметим, что эта внутренняя сумма занимается только выбором таких $k$ человек, которые вытащат подарки, которые сами
и принесли, а мы знаем, что выбрать таких человек можно $\binom{n}{k}$ способами.\\
Таким образом
$\sum\limits_{i_1 < \ldots < i_k} P(A_{i_1} \cap \ldots \cap A_{i_k}) = \binom{n}{k}\frac{(n - k)!}{n!}$. А значит
\[
    P\left( \bigcup\limits_{k = 1}^n A_k \right) = \sum\limits_{k = 1}^n (-1)^{k - 1}
    \sum\limits_{i_1 < \ldots < i_k} P(A_{i_1} \cap \ldots \cap A_{i_k}) =
    \sum\limits_{k = 1}^n (-1)^{k - 1}\binom{n}{k}\frac{(n - k)!}{n!}
\]
Раскроем теперь $\binom{n}{k}$ и упростим:
\[
    P\left( \bigcup\limits_{k = 1}^n A_k \right) =
    \sum\limits_{k = 1}^n (-1)^{k - 1} \binom{n}{k}\frac{(n - k)!}{n!} =
    \sum\limits_{k = 1}^n \frac{(-1)^{k - 1}}{k!}
\]
Таким образом мы посчитали вероятность события, соответсвующего тому, что хотя бы один подарок попал к человеку, который его
принёс. Тогда вероятность того, что не один человек не вытащил свой подарок равна:
\[
    1 - P\left( \bigcup\limits_{k = 1}^n A_k \right) =
    1 - \sum\limits_{k = 1}^n \frac{(-1)^{k - 1}}{k!} =
    1 - 1 + \frac{1}{2} - \frac{1}{3!} + \ldots + \frac{(-1)^{n - 1}}{n!} \to \frac{1}{e}
\]

\subsection{Доказательства существования и задача о конференции.}
Пусть $A_1, \ldots, A_k$ --- произвольные события.\\
Хотим проверить, что $P\left( \bigcap\limits_{k = 1}^n A_k \right) \neq 0$.

Пусть $B_k$ --- событие, противоположное к $A_k$. Тогда
\[
    P\left( \bigcap\limits_{k = 1}^n A_k \right) =
    1 - P\left( \bigcup\limits_{k = 1}^n B_k \right) \geq
    1 - \sum\limits_{k = 1}^n P(B_k)
\]
Т.к. для произвольного набора событий $A_1, \ldots, A_k$ справедлива оценка
\[
    P(A_1 \cup \ldots \cup A_k) \leq \sum\limits_{k = 1}^n P(A_k)
\]
Следует из формулы включений и исключений путём отбрасывания вычитаний попарных пересечений событий и т.д.

Если $\sum\limits_{k = 1}^{n} P(B_k) < 1$, то $\sum\limits_{k = 1}^{n} P\left( \bigcap\limits_{k = 1}^n A_k \right) > 0$.
Т.к. $P(B_k) = 1 - P(A_k)$, то
\[
    \sum\limits_{k = 1}^{n} P(B_k) < 1 \iff
    \left(\sum\limits_{k = 1}^{n} P(B_k) =
    \sum\limits_{k = 1}^{n} \left( 1 - P(A_k) \right) =
    n - \sum\limits_{k = 1}^{n} P(A_k) < 1
    \implies
    \sum\limits_{k = 1}^n P(A_k) > n - 1
    \right)
\]
Так, например если $\forall k$ выполнено $P(A_k) > 1 - \frac{1}{n}$, то $\sum\limits_{k = 1}^n P(A_k) > n - 1$, а значит
пересечении $A_k$ имеет не нулевую вероятность, а следовательно не пусто.
\begin{comment}
    \[
        P(\nothing) = 1 - P(\Omega) = 1 - 1 = 0
    \]
    С другой стороны
    \[
        P(\nothing) = \frac{|\nothing|}{|\Omega|} = \frac{0}{|\Omega|} = 0
    \]
\end{comment}
\begin{problem}[Задача о конференции]
    В лаборатории работаю специалисты по 60 направлениям. По каждому направлению ровно 7 человек (каждый человек может быть
    специалистом по нескольким направлениям). Задача: отправить всех специалистов на две конференции: одна в Канаде,
    другая в Австралии. На каждой конференции должен быть специалист по каждому направлению.

    \it{Решение:} Броском монеты будем для каждого специалиста определять его конференцию. Пусть событие $A_k$ соответствует
    тому, что по $k$-ому направлению есть специалист на обеих конференциях (т.е. $\bigcap\limits_{k = 1}^{60} A_k \neq \nothing$).
    Тогда
    \[
        P(A_k) = 1 - \frac{2}{2^7}
    \]
    где $2^7$ --- количество способов раздать конференцию каждому из $7$ специалистов, а $2$ --- число способов, при которых
    все специалисты уехали либо в Австралию, либо в Канаду. Тогда
    \[
        P(A_k) = 1 - \frac{1}{2^6} > 1 - \frac{1}{60}
    \]
    Видно, что $P(A_k) > 1 - \frac{1}{n}$ при $n = 60$, а значит пересечение $A_k$ не пусто.
    (Мы не предъявляем способ разослать специалистов по конференциям, мы лишь доказываем, что он существует)
\end{problem}

\subsection{Бесконечное множество элементарных исходов и счётная аддитивность.}
Пусть $\Omega$ --- бесконечное множество.
\begin{example}
    $\Omega = \NN$. Определим функцию $P$, удовлетворяющую свойствам вероятностной меры:\\
    пусть $P(A) = 0$, если $A$ --- конечно, и $P(A) = \NN$. Тогда
    \begin{itemize}
        \item $P(\Omega) = P(\NN) = 1$
        \item $A \cap B = \nothing \implies P(A \cup B) = P(A) + P(B) = 0$
    \end{itemize}
    В чём проблема? А вот в чём:
    \[
        \sum\limits_{\omega \in \Omega} P(\{\omega\}) = \sum\limits_{k \in \NN} P(\{k\}) = 0 \neq 1 = P(\NN) = P(\Omega)
    \]
\end{example}
Зададим вероятностную меру следующим множеством: $\{p_{w_j}\;|\; w_j \in \Omega\}$, причём
$\forall\, w_j \in \Omega\colon p_{w_j} \geq 0,\; \sum\limits_{j} p_{w_j} = 1$. Тогда вероястность
каждого события $A$ считается по формуле
\[
    P(A) = \sum\limits_{w_j \in A} p_{w_j}
\]
Заметим, что для вероятностной меры, заданой таким определением выполнено следующее свойство:

\begin{definition}
    Пусть $A_1, A_{2}, \ldots, $ --- произвольный, не более чем счётный набор попарно пересекающихся событий. Тогда
    \[
        P\left( \bigcup\limits_{k = 1}^{\infty}A_k \right) = \sum\limits_{k = 1}^{\infty} P(A_k)
    \]
    Такое свойство мы будем называть \it{счётной аддитивностью}.
\end{definition}
\begin{lemma}[Ликбез в матан]
    \label{lemma_1.1}
    Пусть $a_{n, m}$ --- последовательность положительных чисел. Тогда
    \[
        \sum\limits_{j = 1}^{\infty} a_{\sigma(j)} =
        \sum\limits_{n = 1}^{\infty}\sum\limits_{m = 1}^{\infty}
        a_{n, m}
    \]
    Где $\sigma \colon \NN \to \NN^2$ --- произвольная перестановка.
\end{lemma}
\begin{proof}
    Представим последовательность следующей таблицей:
    \[
        \begin{array}{|c|}
            \hline
            a_{1, 1}~ a_{1, 2}~ \ldots~ a_{1, n_1} \\
            \hline
            a_{2, 1}~ a_{2, 2}~ \ldots~ a_{2, n_2} \\
            \hline
            \vdots                                 \\
            \hline
        \end{array}
    \]
    Тогда сумму элементов в этой таблице мы можем считать построчно: прибавляя к уже накопленной сумме
    каждый следующий элемент. Или же мы можем начать считать эту сумму в рандомном порядке, но при этом
    каждый элемент учитывая только один раз. При этом независимо от того, как именно мы будем складывать,
    результат не должен измениться. В этом и состоит всё утверждение.

    Формально: зафиксируем перестановку $\sigma \colon \NN \to \NN^2$ --- наш способ выбирать элементы
    из таблицы в рандомном порядке. Тогда для произвольных конечных $M, N$ выполено
    \[
        \sum\limits_{j = 1}^{\infty} a_{\sigma(j)} \geq
        \sum\limits_{n = 1}^{N}\sum\limits_{m = 1}^{M} a_{n, m}
    \]
    Тогда устремляя $M$ и $N$ к бесконечности получаем, что
    \[
        \sum\limits_{j = 1}^{\infty} a_{\sigma(j)} \geq
        \sum\limits_{n = 1}^{\infty}\sum\limits_{m = 1}^{\infty}
        a_{n, m}
    \]
    С другой стороны для произвольного конечно $J$ выполнено
    \[
        \sum\limits_{j = 1}^{J} a_{\sigma(j)} \leq
        \sum\limits_{n = 1}^{\infty}\sum\limits_{m = 1}^{\infty} a_{n, m}
    \]
    Устремляя $J$ к бесконечности получаем оценку в другую сторону.

    Из обеих оценок следует равенстсво.
\end{proof}
\begin{proposal}
    Мы не налажали в определении счётной аддитивности.
\end{proposal}
\begin{proof}
    Перепишем утверждение о счётной аддитивности следующим образом:
    \[
        P\left( \bigcup\limits_{k = 1}^{\infty}A_k \right) = \sum\limits_{k = 1}^{\infty} P(A_k)
        \iff
        \sum\limits_{w_i \in \bigcup\limits_{k = 1}^{\infty}A_k} p_{w_j} =
        \sum\limits_{k = 1}^{\infty}\sum\limits_{w_j \in A_k} p_{w_j}
    \]
    Справедливость полученного утверждения следует из \hyperref[lemma_1.1]{доказанной леммы}.
\end{proof}
\subsection{Условная вероятность, формула полной вероятности
и формула Байеса.}
\begin{definition}
    Пусть $P(B) > 0$. Вероятность события $A$ при условии события $B$, определяемая как
    \[
        P(A | B) = \frac{P(A \cap B)}{P(B)}
    \]
    называется \it{условной вероятностью}.

    При фиксированном $B$ функция $P(\cdot | B)$ является новой вероятностной мерой.
\end{definition}
\begin{definition}
    \it{Правилом произведения} называется следующее равенство:
    \[
        P(A \cap B) = P(A | B) \cdot P(B)
    \]
\end{definition}
\begin{theorem}[Формула полной вероятности.]
    Пусть $\Omega = A_1 \cup A_2 \cup \ldots \cup A_n$ и $A_i \cap A_j = \nothing\;\; \forall\;
    i \neq j$. Пусть так же $\forall\; i\colon A_i \neq \nothing$. Тогда для всякого события $B$
    верно
    \[
        P(B) = \sum\limits_{i} P(B|A_i)P(A_i)
    \]
\end{theorem}
\begin{proof}
    Имеем
    \[
        P(B) = \sum\limits_{i = 1}^{n} P(B \cap A_i) =
        \sum\limits_{i = 1}^{n} P(B|A_i) \cdot P(A_i)
    \]
\end{proof}
\begin{theorem}[Формула Байеса.]
    Пусть $A, B$ --- два непустых события. Тогда верна формула Байеса:
    \[
        P(A|B) = \frac{P(B|A)P(A)}{P(B)}
    \]
\end{theorem}
\begin{proof}
    Имеем
    \[
        P(A|B)P(B) = P(A \cap B) = P(B \cap A) = P(B|A)P(A)
    \]
\end{proof}

\subsection{Независимые события. Попарная независимость и независимость в совокупности.}
\begin{definition}
    События $A$ и $B$ называются \it{независимыми}, если выполнено
    \[
        P(A \cap B) = P(A) \cdot P(B)
    \]
    иначе события называются \it{зависимыми}.
\end{definition}
\begin{definition}
    События $A_1, A_2, \ldots, A_n$ называются \it{независимыми в совокупности}, если
    для некоторого $k \in \{2, \ldots, n\}$ и произвольных $1 \leq i_1 < i_2 < \ldots < i_k \leq n$
    выполнено
    \[
        P(A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}) =
        P(A_{i_1}) \cdot \ldots \cdot P(A_{i_k})
    \]
\end{definition}
\begin{proposal}
    Независимость в совокупности не совпадает с попарность независимостью.
\end{proposal}
\begin{proof}
    Будем два раза подкидывать монету. Пусть событие $A$ --- при первом бросании выпал орёл;
    событие $B$ --- при втором бросании выпал орёл; событие $C$ --- орёл выпал ровно один раз.
    Очевидно, что эти события попарно независимы. Однако если при обоих бросках выпали орлы, то
    событие $C$ становится невозможным, а значит\\ $P(A \cap B \cap C) = 0 \neq P(A) \cdot P(B)
    \cdot P(C)$.
\end{proof}

\subsection{Задача о билетах к экзамену.}
\begin{problem}
    Программа коллоквиума состоит из $N$ билетов, а студент выучил только $n$. На колке студенты
    по очереди подходят и тянут билет. Зависит ли вероястность вытянуть "хороший" \ билет от места
    в очереди?

    \it{Решение:} Пусть студент стоит на $k + 1$ месте в очереди и пусть событие $A$ ---
    студент вытянул "хороший" \ билет. Положим событие $A_j$ --- первые $k$ студентов вытянули
    $j$ "хороших" \ билетов; Событие $B$ --- вытянуть выученный билет. Тогда очевидно, что
    \[
        P(B) = \sum\limits_{j} P(B | A_j) P(A_j)
    \]
    Заметим, что вероятность $P(B | A_j)$ считается очень просто: действительно, когда подойдёт наша очередь,
    Останется всего $n - j$ подходящих нам билетов, а всего билетов останется $N - k$. Таким образом
    $P(B |A_j) = \frac{n - j}{N - k}$.

    Найдём теперь $P(A_j)$: заметим, что до нашей очереди билеты были выбраны $\binom{N}{k}$ способами.
    При этом известно, что было вытянуто $j$ выученных билетов, это можно сделать $\binom{n}{j}$ способами.
    Значит, из невыученных билетов (их $N - n$) осталось вытянуть $k - j$ билетов. Это можно сделать
    $\binom{N - n}{k - j}$ способами. Итого имеем:
    \[
        P(A_j) = \frac{\binom{n}{j} \cdot \binom{N - n}{k - j}}{\binom{N}{k}}
    \]
    Подставим в формулу полной вероятности:
    \begin{align*}
        P(B) &= \sum\limits_{j} P(B | A_j) P(A_j) =
        \sum\limits_{j} \frac{n - j}{N - k} \cdot
        \frac{\binom{n}{j} \cdot \binom{N - n}{k - j}}{\binom{N}{k}} =
        \sum\limits_{j} \frac{n - j}{N - k} \cdot
        \frac{\frac{n!}{(n - j)!j!} \cdot \binom{N - n}{k - j}}{\frac{N!}{(N - k)!k!}} =\\
        &=\sum\limits_{j} \binom{N - n}{k - j} \cdot \frac{n - j}{N - k} \cdot
        \frac{n!}{(n - j)!j!} \cdot \frac{(N - k)!k!}{N!} =
        \sum\limits_{j} \binom{N - n}{k - j} \frac{n!}{(n - j - 1)!j!}
        \cdot \frac{(N - k - 1)!k!}{N!} =\\
        &=\frac{n}{N}\sum\limits_{j}\binom{N - n}{k - j} \frac{(n - 1)!}{(n - j - 1)!j!}
        \cdot \frac{(N - k - 1)!k!}{(N - 1)!} =
        \frac{n}{N}\sum\limits_{j}\underbrace{\binom{N - n}{k - j} \cdot \binom{n - 1}{j}
        \cdot \left[ \binom{N - 1}{k} \right]^{-1}}_{P(A_j) \text{ для меньшего числа билетов}}=
        \frac{n}{N}
    \end{align*}
    Т.к. $A_j$ попарно не пересекаются, и в объединении дают $\Omega$, то $\sum P(A_j) = 1$ по свойству вероятностной меры.

    С другой стороны, если бы билеты раздавались случайным образом, то очевидно, что вероятность
    получить среди $N$ билетов один из $n$ выученных равна $\frac{n}{N}$. Отсюда делаем вывод,
    что позиция в очереди никак не влияет на вероятность вытянуть выученный билет, и вообще нет
    разницы просто с рандомной раздачей билетов.
\end{problem}

\subsection{Задача о сумасшедшей старушке.}
\begin{problem}
    На посадку в самолёт стоят $N \geq 2$ пассажиров, среди которых есть сумасшедшая старушка. Старушка
    расталкивает остальных пассажиров и садится в самолёт на произвольное место. Затем пассажиры, когда заходят в
    самолёт, садятся на своё место, если оно свободно, если же место занято, то пасажир садится на рандомное
    место. Какова вероятность того, что последний пассажир сядет на своё место?

    \it{Решение} Обозначим вероятность $N$-ого пассажира сесть на своё место за $P_N$. Индукция по $N$:\\
    База: для $N = 2$ вероятность, очевидно, $\frac{1}{2}$.\\
    Предположение: пусть для $N = k$ верно, что $P_N = \frac{1}{2}$.\\
    Шаг: докажем для $N +1$: пусть событие $B$ --- последний пассажир сел на своё место; событие $A_j$ ---
    старушка села на место $j$-ого пассажира. Тогда по формуле полной вероятности:
    \[
        P(B) = \sum\limits_{i} P(B | A_i) \cdot P(A_i)
    \]
    Заметим, что $P(B)$ есть то же самое, что и $P_{N + 1}$. Кроме того, т.к. в самолёт входят $N + 1$
    пассажир, а старушка садится на любое место, то для любого $i$ верно $P(A_i) = \frac{1}{N + 1}$.
    Распишем теперь $P(B)$ следующим образом:
    \begin{align*}
        &P_{N + 1} = P(B) = \sum\limits_{i = 1}^{N + 1} P(B | A_i) \cdot P(A_i) =
        P(B | A_1)P(A_1) + \sum\limits_{i = 2}^{N} P(B | A_i) \cdot P(A_i) + P(B | A_{N + 1})P(A_{N + 1})
    \end{align*}
    Разбираемся: событие $A_1$ есть событие, при котором старушка садится на своё место (т.к. её саму
    можно считать первым пассажиром), тогда $P(B | A_1) = 1$, т.к. если старушка села на своё место, то все
    последующие пассажиры так же сядут на своё место, включая последнего. С другой стороны, если произошло
    событие $A_{N + 1}$, т.е. старушка села на место последнего пассажира, то, очевидно, $P(B | A_{N + 1}) = 0$.
    Для всех остальных $i$ по предположению индукции верно: $P(B | A_i)P(A_i) = \frac{1}{2}$.
    \begin{align*}
        &P_{N + 1} = P(B | A_1)P(A_1) + \sum\limits_{i = 2}^{N} P(B | A_i) \cdot P(A_i) +
        P(B | A_{N + 1})P(A_{N + 1}) =
        \frac{1}{N + 1}\left( 1 + \sum\limits_{i = 2}^{N} \frac{1}{2} + 0 \right) =
        \frac{1}{N + 1}\left( 1 + \frac{N - 1}{2} \right) =\\
        &=\frac{1}{N + 1} + \frac{N - 1}{2(N + 1)} =\frac{2 + N - 1}{2(N + 1)} = \frac{1}{2}
    \end{align*}
\end{problem}

\subsection{Парадокс Байеса.}
Имеется тест для диагностики некоторого редкого заболевания. Известно, что доля больных этим
заболеванием равна $0.001$. Если человек болен, то тест даёт положительный результат с вероятностью $0.99$.
Если человек здоров, то тест даёт положительный результат с вероятностью $0.01$.
Требуется найти вероятность ложноположительного результата.
\it{Решение:} пусть события $T_{-}$ и $T_{+}$ --- тест дал отрицательный и положительный результаты
соответственно; события $Z_{-}$ и $Z_{+}$ --- человек здоров или болен.\\
Найдём вероятность положительного теста по формуле полной вероятности:
\[
    P(T_{+}) = P(T_{+} | Z_{+})P(Z_{+}) + P(T_{+} | Z_{-})P(Z_{-}) = 0.99 \cdot 0.001 + 0.01 \cdot 0.999 =
    0.01098
\]
Теперь по формуле Байеса найдём вероятность того что человек здоров при условии, что
тест дал положительный результат:
\[
    P(Z_{-} | T_{+}) = \frac{P(T_{+} | Z_{-})P(Z_{-})}{P(T_{+})} = \frac{0.01 \cdot 0.999}{0.01098} \to 1
\]
Таким образом видно, что из-за редкости заболевания тест почти гарантированно даст ложноположительный результат.

\subsection{Парадокс Монти Холла.}
Теперь мы учавствуем в игре, в которой нужно выбрать одну из трёх дверей. За одной из дверей находится автомобиль,
а за двумя другими --- козы (а что, я бы и от козы не отказался...). После того как мы выбираем одну дверь
(пусть будет первая), ведущий равновероятно открывает одну из оставшихся дверей (пусть будет третью),
за которой находится коза (важно: если мы изначально выбрали дверь
с козой, то ведущий просто откроет другую дверь с козой. Атомобиль он нам не покажет). После этого мы имеем возможность
изменить свой выбор на вторую дверь. Следует ли нам это делать?

\it{Решение:} Пусть событие $A_i$ --- автомобиль находится за $i$-ой дверью. Очевидно, что
$\forall\; i\; P(A_i) = \frac{1}{3}$. Пусть событие $B$ --- ведущий открыл $3$-ю дверь. Тогда если верно
событие $A_1$, то $P(B | A_1) = \frac{1}{2}$. Если же событие $A_1$ не верно, а автомобиль находится, например,
за 3 дверью, то $P(B | A_2) = 1$, $P(B | A_3) = 0$. Аналогично для события $A_2$. Тогда, по формуле Байеса найдём
вероятность того, что автомобиль находится за 2 дверью, при условии, что ведущий открыл третью дверь:
\[
    P(A_2 | B) = \frac{P(B | A_2)P(A_2)}{P(B)} =
    \frac{P(B | A_2)P(A_2)}{P(B | A_1)P(A_1) + P(B | A_2)P(A_2) + P(B | A_3)P(A_3)} = \frac{1/3}{1/6 + 1/3 + 0} =
    \frac{2}{3}
\]
С другой стороны изначально шанс угадать, где находится автомобиль, был равен $\frac{1}{3}$, а значит, согласившись
изменить выбор двери, мы повысим шанс получить автомобиль.